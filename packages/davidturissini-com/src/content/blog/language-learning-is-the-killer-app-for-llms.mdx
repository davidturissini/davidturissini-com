---
title: Language Learning is the Killer App for LLMs
image: ../../images/language-learning-is-the-killer-app-for-llms.webp
---

My kids have been getting more and more into video games. A few nights a week, we will sit down together and play games on our Switch. Every once in a while, I show them some old Super Nintendo games that I played growing up. It has been an absolute joy to share these games with them and I was particularly excited to share [Super Mario World 2: Yoshi's Island](https://en.wikipedia.org/wiki/Yoshi%27s_Island) with them.

Yohsi's Island is a game where you control Yoshi and help save Baby Mario from Baby Browser. It has a ton of really fun elements to it, but one of the most unique aspects to this game is that Yoshi can transform into different vehicles that give him special powers. He can turn into a helicopter and fly around. Or he can turn into a car and reach places that "regular" Yoshi wouldn't fit into. He can even turn into a train that follows tracks and collects items.

Transforming Yoshi into vehicles is a lot of fun, but its only possible in a few levels in the game. Trains are a popular toy in our house, so I wanted to show my kids that Yoshi can transform into a train. The problem was that I hadn't played this game in years so I couldn't remember the exact level where the transform was. So, I took out my phone, opened ChatGPT and asked:

> In yoshi’s island on SNES which level can you turn into a train

ChatGPT replied with:

> In Yoshi's Island for the SNES, you can turn into a train in several levels. One of the most notable levels is World 2-3, titled "What's Gusty Taste Like?" In this level, there is a section where Yoshi can transform into a train to navigate through the level on tracks.

Great! We loaded up the level, played through and looked for the transform. We looked. And Looked. And Looked. There was no train transform in World 2-3¹. 

ChatGPT was wrong.

## LLM Factual Accuracy

We've all had moments where ChatGPT or Claude gives us a response that seems correct but is actually incorrect. This can happen in subtle ways too, where the generated output is mostly correct but gets some important or minor detail wrong. This makes it difficult to trust LLM output, especially in areas outside of your expertise. How can you trust that the LLM output is correct or not?

## Gramatical Accuracy

It is difficult to trust factual accuracy of LLM output, but looking back at ChatGPT's incorrect response I realized something. The response was not factually correct, but it _was_ gramatically correct. I didn't have to read the sentence two or three times to understand what it mean. It uses plain language and even has correct comma, hyphen, and quotation mark usage. Even though the facts were wrong, the grammar was very accurate. What I realized further was that I couldn't think of a time when ChatGPT or Claude had given me _anything_ that was gramatically incorrect.  

It turns out that LLMs have a high degree of gramatical accuracy, and gramatical accuracy is far more important than factual accuracy when learning a new language. This is especially true at the lower levels. Every story in the A1-A2 range, and even some in the B1 and B2 levels, are be designed to orient readers to their target language's grammar. The facts of the story really do not matter beyond comprehension testing. None of the LLMs on the market today have any trouble generating gramatically correct content in many, many languages.

## Word Choice

Gramatical accuracy is important but it is only one piece of the puzzle. As a beginning student of French, it wouldn't do me any good to try to read a text at a C1 level. As a more advanced student of Dutch, I would be absolutely bored to read materials designed for an A1 student. Fortunately, LLMs understand language learning levels and their outputs can be calibrated by level. Compare the following prompts:


**Prompt for A1 students:**
> You are an ESL teacher. You teach American English. Your students are all at an A1 level. Generate a short story for your students. It should be 6 sentences long.


**Output:**
> Sarah has a small, black cat named Luna. One day, Luna runs away from the house. Sarah looks for Luna in the garden, but she cannot find her. She asks her neighbor, "Have you seen my cat?" The neighbor says, "Yes! Luna is sleeping under the tree." Sarah is happy and takes Luna home.


**Prompt for B2 students:**
> You are an ESL teacher. You teach American English. Your students are all at an B2 level. Generate a short story for your students. It should be 6 sentences long.


**Output:**
> Maria had been feeling stressed all week because of her new job. On Friday evening, just as she was about to leave the office, she found a small box on her desk. Surprised, she opened it and saw a beautiful bracelet with a note that read, "You’re doing great, keep going!" She smiled, instantly feeling lighter and more motivated. Maria had no idea who left the gift, but the gesture made her day. As she walked home, she realized that sometimes small acts of kindness can make a big difference.

It's easy to see the differences between these two texts. The A1 result uses A1 appropriate words and simple, A1 appropriate grammar. Predictably, the B2 response includes more complex grammar structures and more complex words that are appropriate for a B2 student. We could modify these prompts for any English level we desired. 

## Progression

Beyond generating stories, LLMS can also be very useful for moving you through a language. Instead of immediately moving from A2 to B1 texts, LLMs can smooth that transition by creating materials that very gradually begin to introduce new concepts without overwhelming the learner. Consider this prompt:

> I understand "I went to the store yesterday to buy some milk and bread." but not "Although I had initially planned to attend the meeting, unforeseen circumstances arose, forcing me to change my schedule at the last minute.". Generate a sentence that is more challenging than the first but easier than the second.

**Output:**
> Here’s a sentence that is slightly more advanced than the first but easier than the second:

> "I wanted to go to the park, but it started raining, so I stayed home instead."

> This sentence introduces a conjunction ("but") and a reason clause, making it a bit more complex than A2 while still being easier to follow than a B2-level sentence.

We are able to not only give the LLM a target level to use, but we can also ask it to make something _slightly_ more difficult than something else you already know. It can even tell you _why_ the final sentence is between both levels.

## Conclusion

Using Large Language Models to generate language learning materials leans in to LLM's strength while completely side-stepping their biggest weakness: factual accuracy. LLMs can reliably generate gramatically correct language learning materials with level-appropriate vocabulary. Additionally, LLMs can guide the language learner gradually through the learning process.

While I will think twice the next time I ask ChatGPT about the location of video game items, I will not hesitate to ask ChatGPT to help me learn whatever language I happen to be learning.


<ol class="text-slate-400 text-xs m-0 p-0">
  <li class="list-none p-0">¹ World 2-3 has a <a class="text-inherit" href="https://mario.fandom.com/wiki/Car_(form)" target="_BLANK">car transform</a></li>
</ol>